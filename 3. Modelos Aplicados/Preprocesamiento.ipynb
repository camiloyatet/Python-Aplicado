{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento se refiere a las transformaciones aplicadas a nuestros datos antes de alimentarlos al algoritmo. El preprocesamiento de datos es una técnica que se utiliza para convertir los datos sin procesar en un conjunto de datos limpio. En otras palabras, cada vez que los datos se recopilan de diferentes fuentes, se recopilan en formato sin procesar que no es factible para el análisis.\n",
    "\n",
    "__Importancia del preprocesamiento__\n",
    "\n",
    "* Para lograr mejores resultados del modelo de Machine Learning, el formato de los datos debe estar de manera adecuada.\n",
    "* Algunos modelos específicos de Machine Learning necesitan información en un formato específico, por ejemplo, algunos de ellos no admiten valores nulos, por lo tanto, es necesario limpiarlos\n",
    "\n",
    "<img align=\"center\" width=\"800\" height=\"300\" src=\"https://www.geeksforgeeks.org/wp-content/uploads/ml.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy  as np\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"GnBu_d\")\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"100\" height=\"300\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/87736c83415dcd9fdcff9b416a246e72bf83fff7\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # Crear un objeto de escala\n",
    "scaler.fit(titanic[[\"Age\", \"Fare\"]])\n",
    "titanic[['Age2', 'Fare2']]=pd.DataFrame(scaler.transform(titanic[[\"Age\", \"Fare\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"300\" height=\"300\" src=\"https://qph.fs.quoracdn.net/main-qimg-0d692d88876aeb26b1f1a578d1c5a94e.webp\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() # Crear un objeto de escala\n",
    "scaler.fit(titanic[[\"Age\", \"Fare\"]])\n",
    "titanic[['Age2', 'Fare2']]=pd.DataFrame(scaler.transform(titanic[[\"Age\", \"Fare\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones no Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación Quantil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte  la variable a una distribución deseada mediante la formula $$G^{-1}(F(X))$$ donde $F$ es la función acumulativa de la distribución de la variable a transformar y $G^{-1}$ es la función cuantil de la distribución necesaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = QuantileTransformer(random_state= 101, n_quantiles=100, output_distribution='normal')\n",
    "scaler.fit_transform(titanic[[\"Age\", \"Fare\"]])\n",
    "titanic[['Age2', 'Fare2']]=pd.DataFrame(scaler.transform(titanic[[\"Age\", \"Fare\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Age2',y='Age',data=titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Fare2',y='Fare',data=titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacion de Potencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos escenarios de modelado, es deseable la normalidad de las variables. Las transformaciones de potencia son una familia de transformaciones paramétricas y monotónicas que tienen como objetivo mapear datos de cualquier distribución lo más cerca posible de una distribución gaussiana para estabilizar la varianza y minimizar la asimetría. Se suele usar la transformacion de Yeo-Johnson\n",
    "\n",
    "$$ \\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    " [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n",
    "\\ln{(x_i + 1)} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n",
    "-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n",
    " - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "O de Box-Cox\n",
    "\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n",
    "\\ln{(x_i)} & \\text{if } \\lambda = 0,\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "Donde $\\lambda$ es el parámetro de máxima verosimilitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "scaler.fit_transform(titanic[[\"Age\", \"Fare\"]])\n",
    "titanic[['Age2', 'Fare2']]=pd.DataFrame(scaler.transform(titanic[[\"Age\", \"Fare\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Age2',y='Age',data=titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Fare2',y='Fare',data=titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación de datos Categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn no admite datos categóricos en sus algortimos, por esta razón es indispensable transformarlos mediante codificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder (Dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir las variables categóricas se logra mediante lo que se conoce como One Hot Encoding. Este tipo de codificación se puede obtener con OneHotEncoder, que transforma cada nivel de la variable categórica en variables binarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ= ['Sex', 'Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.get_dummies(titanic,columns=categ,drop_first=True)\n",
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación Univariada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(titanic[[\"Age\"]])\n",
    "titanic[['AgeImp']]=pd.DataFrame(imp.transform(titanic[[\"Age\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp.fit(titanic[[\"Cabin\"]])\n",
    "titanic[['CabinImp']]=pd.DataFrame(imp.transform(titanic[[\"Cabin\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación Multivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un enfoque más sofisticado es usar la clase IterativeImputer, que modela cada variable con valores faltantes en función de otras variables, y usa esa estimación para la imputación. Lo hace de forma iterativa: en cada paso, una columna de características se designa como salida y las otras columnas de características se tratan como entradas X. Se ajusta un regresor en (X, y) para y conocido. Luego, el regresor se usa para predecir los valores faltantes de y. Esto se hace para cada entidad de forma iterativa, y luego se repite para rondas de imputación max_iter. Se devuelven los resultados de la ronda final de imputación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=100, random_state=0)\n",
    "imp.fit(titanic[[\"Age\"]])\n",
    "titanic[['AgeImp']]=pd.DataFrame(imp.transform(titanic[[\"Age\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se basa en el paquete de R MICE (Multivariate Imputation by Chained Equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación por KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proporciona la imputación para completar los valores perdidos utilizando el enfoque k-Nearest Neighbours. Por defecto, se usa una métrica de distancia euclidiana. Cada valor faltante se imputa utilizando valores de los k vecinos más cercanos que tienen un valor para la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imp = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imp.fit(titanic[[\"Age\"]])\n",
    "titanic[['AgeImp']]=pd.DataFrame(imp.transform(titanic[[\"Age\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Componentes Prinicipales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información. Supóngase que existe una muestra con $n$ individuos cada uno con $p$ variables $(X_1, X_2, …, X_p)$. PCA permite encontrar un número de factores subyacentes $(z<p)$ que explican aproximadamente lo mismo que las $p$ variables originales. Donde antes se necesitaban $p$ valores para caracterizar a cada individuo, ahora bastan $z$ valores. Cada una de estas z nuevas variables recibe el nombre de componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/samples/titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imp = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imp.fit(titanic[[\"Age\"]])\n",
    "titanic[['Age']]=pd.DataFrame(imp.transform(titanic[[\"Age\"]]))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.select_dtypes(include=[\"number\"]).drop(\"PassengerId\", axis=1).drop(\"Survived\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=n_comp, svd_solver='randomized', whiten=True).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca= pd.DataFrame(pca.transform(X), columns=[\"PC%d\" % k for k in range(1,n_comp + 1)])\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pca.explained_variance_ratio_:\n",
    "  print(\"Varianza de la componente: \"+\"{:.2%}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sobrevive = pd.Series(titanic.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_list = [{0:\"r\",1:\"g\",2:\"b\"}[x] for x in Sobrevive]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=X_pca[\"PC1\"], y=X_pca[\"PC2\"], color=color_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El PCA implica distancias, por lo cual es necesario rescalar las variables. Si se rescalan las variables, como cambia el porcentaje de varianza explicado?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
